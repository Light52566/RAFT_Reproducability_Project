{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAqfwltQKwsJ"
      },
      "source": [
        "# Initialization\n",
        "* Directory set\n",
        "* Library import\n",
        "* Dataset download\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Xu7Jp_t98QWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nst-eW4GFDHw",
        "outputId": "8bb9156d-9c4b-414a-fcd9-505e48c650e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "#you can save your own drive project path here as a comment\n",
        "#'/content/gdrive/MyDrive/TUDelft/1.3/DeepLearning/DeepLearningProject'\n",
        "#'/content/gdrive/MyDrive/DeepLearningProject'\n",
        "drive.mount('/content/gdrive')\n",
        "sys.path.append('/content/gdrive/MyDrive/TUDelft/1.3/DeepLearning/DeepLearningProject')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q0jbAPu-L-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29660e6f-f9b6-47d1-df04-f3dabccafda7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1JFdBnQ-FX1S6xSzRbNQ95iJYaDMD5y9Z/DeepLearningProject\n"
          ]
        }
      ],
      "source": [
        "#cd /content/gdrive/MyDrive/DeepLearningProject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHo8YL6FITKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df2b5c9-26de-4519-811e-221339505ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/gdrive/MyDrive/TUDelft/1.3/DeepLearning/DeepLearningProject/'\n",
            "/content/gdrive/.shortcut-targets-by-id/1JFdBnQ-FX1S6xSzRbNQ95iJYaDMD5y9Z/DeepLearningProject\n"
          ]
        }
      ],
      "source": [
        "cd /content/gdrive/MyDrive/TUDelft/1.3/DeepLearning/DeepLearningProject/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb4QhhheIcM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f9f905-7c66-48d1-efc6-6a947485688c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2003.12039.pdf  'Project Instructions.gdoc'  'RAFT: Reproduction Plan.gdoc'\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/            \u001b[01;34m__pycache__\u001b[0m/                 ReproducibilityCode\n",
            " encoders.py     'Raft Architecture.drawio'    TestCopyReproducibilityCode\n",
            " \u001b[01;34mmodels\u001b[0m/         'Raft Architecture.png'\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpb5ldLSExIQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "from torchvision.datasets import CIFAR10, Sintel, FlyingChairs\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.cuda.amp import GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn4Fe6du_F9a"
      },
      "outputs": [],
      "source": [
        "# Transformations applied on each image => only make them a tensor\n",
        "DATASET_PATH = \"./data\"\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,),(0.5,))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WpbmskdhMT0"
      },
      "outputs": [],
      "source": [
        "class LoadFlow(transforms.ToTensor):\n",
        "    def __call__(self, img1, img2, flow, valid_flow_mask):\n",
        "        return (T.functional.to_tensor(img1), T.functional.to_tensor(img2), flow, valid_flow_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHHwsXE_FBjJ"
      },
      "outputs": [],
      "source": [
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "#train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
        "train_dataset = Sintel(root=DATASET_PATH, split = 'train', pass_name = 'clean', transforms = LoadFlow())\n",
        "#test_dataset = Sintel(root=DATASET_PATH, split = 'test', pass_name = 'clean', transforms = LoadFlow())\n",
        "#train_dataset = FlyingChairs(root=DATASET_PATH, split = 'test', transforms = LoadFlow())\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=True, pin_memory=True, num_workers=2)\n",
        "#test_loader = data.DataLoader(test_dataset, batch_size=2, shuffle=False, drop_last=True, pin_memory=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoders\n",
        "\n",
        "\n",
        "\n",
        "*   ResidualBlock\n",
        "*   Encoder\n",
        "*   Upscaler\n",
        "\n"
      ],
      "metadata": {
        "id": "xok5gEN848F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUnits(nn.Module):\n",
        "    \n",
        "  def __init__(self,\n",
        "                num_input_channels : int,\n",
        "                num_output_channels : int,\n",
        "                stride : int,\n",
        "                num_groups : int = 16,\n",
        "                norm : object = nn.InstanceNorm2d):\n",
        "      super().__init__()\n",
        "\n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      # self.norm = norm(num_output_channels)\n",
        "\n",
        "      self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=num_output_channels)\n",
        "      self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=num_output_channels)\n",
        "      self.norm3 = nn.GroupNorm(num_groups=num_groups, num_channels=num_output_channels)\n",
        "\n",
        "      self.stride = stride\n",
        "\n",
        "      self.downscale = nn.Sequential(nn.Conv2d(num_input_channels, num_output_channels, 1, stride=stride),self.norm3)\n",
        "\n",
        "      self.net1 = nn.Conv2d(num_input_channels, num_output_channels, 3, stride=stride, padding=1)\n",
        "      self.net2 = nn.Conv2d(num_output_channels, num_output_channels, 3, padding=1)\n",
        "        \n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.net1(x)\n",
        "      out = self.norm1(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      out = self.net2(out)\n",
        "      out = self.norm2(out)\n",
        "      out = self.relu(out)\n",
        "\n",
        "      if self.stride != 1:\n",
        "        x = self.downscale(x)\n",
        "\n",
        "      out = self.relu(x+out)\n",
        "      \n",
        "      return out"
      ],
      "metadata": {
        "id": "ynei2g7S5QSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_input_channels : int = 3,\n",
        "                 base_channel_size : int = 32,\n",
        "                 num_groups : int = 16,\n",
        "                 num_output_channels : int = 128,\n",
        "                 norm : object = nn.InstanceNorm2d):\n",
        "        \n",
        "      super().__init__()\n",
        "      \n",
        "      self.relu = nn.ReLU(inplace=True)\n",
        "      channels = base_channel_size\n",
        "      # self.norm = norm(base_channel_size)\n",
        "      \n",
        "      self.norm = nn.GroupNorm(num_groups=num_groups, num_channels=channels)\n",
        "\n",
        "      self.init_convolution = nn.Conv2d(num_input_channels, channels, 7, stride=2, padding=3)\n",
        "      residual_half = nn.Sequential(ResidualUnits(channels, channels, stride=1), ResidualUnits(channels, channels, stride=1))\n",
        "      residual_quarter = nn.Sequential(ResidualUnits(channels, 2*channels, stride=2), ResidualUnits(2*channels, 2*channels, stride=1))\n",
        "      residual_eigth = nn.Sequential(ResidualUnits(2*channels, 3*channels, stride=2), ResidualUnits(3*channels, 3*channels, stride=1))\n",
        "      self.final_convolution = nn.Conv2d(3*channels, num_output_channels, 1)\n",
        "\n",
        "      self.net = nn.Sequential(\n",
        "        residual_half,\n",
        "        residual_quarter,\n",
        "        residual_eigth\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.init_convolution(x)\n",
        "      x = self.norm(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.net(x)\n",
        "      \n",
        "      return self.final_convolution(x)"
      ],
      "metadata": {
        "id": "Q-RtO6LT5VBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnfoldDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "                num_input_channels : int,\n",
        "                base_channel_size : int = 32,\n",
        "                act_fn : object = nn.ReLU):\n",
        "      \n",
        "    super().__init__()\n",
        "\n",
        "    c_hid = base_channel_size\n",
        "    self.output = num_input_channels\n",
        "\n",
        "    self.mask = nn.Sequential(\n",
        "      nn.Conv2d(96, 4*c_hid, 3, padding=1),\n",
        "      act_fn(),\n",
        "      nn.Conv2d(4*c_hid, 8*8*9, 1)\n",
        "    )\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=4)\n",
        "\n",
        "  def forward(self, x, flow):\n",
        "    # print(x.shape)\n",
        "    B, D, W, H = x.shape # SANDER: careful! the order is [B, D, H, W] actually\n",
        "    # SANDER: it works... because you are consistent in naming them wrong ^^ :)\n",
        "    # SANDER: in my comments below I have also adopted your swapped H/W logic...\n",
        "\n",
        "    mask = self.mask(x)\n",
        "    \"\"\" SANDER: mask is [B, D, W, H]\"\"\"\n",
        "\n",
        "    # print(mask.shape)\n",
        "    mask = mask.view(B, 1, 9, 8, 8, W, H)\n",
        "    mask = self.softmax(mask)\n",
        "\n",
        "    flow = F.unfold(flow, 3, padding=1) # SANDER: [B, 2, 9, 1, 1, W, H] (right?) (FIXED)\n",
        "    # SANDER: I believe this goes wrong; you are only allowed to take a different view\n",
        "    # SANDER:   when the order remains the same!\n",
        "    # SANDER: consider flow to be [B, 2, 9, 1, 1, W, H]\n",
        "    # SANDER: then viewing [B*2*9*1*1, W, H] from 'flow' would be safe\n",
        "    # SANDER: so would [B*2, 9*1, 1, W, H]\n",
        "    # SANDER: but [B*9, 2*1*1, W, H] would **NOT**\n",
        "    # SANDER: permute first, before taking a different view or reshaping\n",
        "    \n",
        "    y = flow.view(B, self.output, 9, 1, 1, W, H)\n",
        "\n",
        "    y = torch.sum(mask * y, dim=2)\n",
        "\n",
        "    # \n",
        "    return y.reshape(B, -1, 8*W, 8*H)"
      ],
      "metadata": {
        "id": "zeGRtKsI5Y_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9RxwP_gLwmo"
      },
      "source": [
        "# Computing Visual Similarity\n",
        "* Correlation Volume\n",
        "* Correlation Pyramid\n",
        "* Correlation Lookup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93HCvs_zKDFa"
      },
      "outputs": [],
      "source": [
        "#Compute correlation volume C between f1 and f2 image features using torch\n",
        "#correlation volume formed by dot product between all pairs of feature vectors in f1 and f2\n",
        "#input: f1, f2: feature maps of size BxDxWxH\n",
        "#output: correlation volume C of size BxWxHxWxH\n",
        "def correlationVolume(f1, f2):\n",
        "    # Calculate the tensor product using einsum\n",
        "    C = torch.einsum('bdij,bdkl->bijkl', f1, f2)\n",
        "    return C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmiPUEs6KFq_"
      },
      "outputs": [],
      "source": [
        "#Construct a 4-layer pyramid {C1, C2, C3, C4} by pooling the last two dimensions using torch:\n",
        "# input: C: correlation volume of size BxWxHxWxH\n",
        "# output: [C_k]: has dimensions: B x W x H x W/2^k x H/2^k for k = 1, 2, 3, 4\n",
        "def constructPyramid(C):\n",
        "    #do pooling for each batch\n",
        "    for i in range(C.size()[0]):\n",
        "        C1_batch = C[i, :, :, :, :]\n",
        "        # SANDER: the paper uses avg_pool, not max pool (FIXED)\n",
        "        C2_batch = torch.nn.functional.avg_pool2d(C1_batch, 2, stride=2)\n",
        "        C3_batch = torch.nn.functional.avg_pool2d(C2_batch, 2, stride=2)\n",
        "        C4_batch = torch.nn.functional.avg_pool2d(C3_batch, 2, stride=2)\n",
        "        #combine the batch results on new batch dimension again\n",
        "        if i == 0:\n",
        "            C1 = C1_batch.unsqueeze(0)\n",
        "            C2 = C2_batch.unsqueeze(0)\n",
        "            C3 = C3_batch.unsqueeze(0)\n",
        "            C4 = C4_batch.unsqueeze(0)\n",
        "        else:\n",
        "            C1 = torch.cat((C1, C1_batch.unsqueeze(0)), 0)\n",
        "            C2 = torch.cat((C2, C2_batch.unsqueeze(0)), 0)\n",
        "            C3 = torch.cat((C3, C3_batch.unsqueeze(0)), 0)\n",
        "            C4 = torch.cat((C4, C4_batch.unsqueeze(0)), 0)\n",
        "    return [C1, C2, C3, C4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0okssJ3X7hd"
      },
      "outputs": [],
      "source": [
        "#Compute the feature map F using the pyramid P and the flow field estimate fe using torch\n",
        "#input: P: pyramid of size [C1, C2, C3, C4] where each Ck has dimensions BxWxHxW/2^kxH/2^k\n",
        "#       fe: flow field estimate of size Bx2xWxH where dim 1 is the x and y components\n",
        "#output: F: feature map of size Bx4(2r+1)**2xWxH\n",
        "def createFeatureMap(correlation_pyramid, fe, radius=4):\n",
        "  B, W, H, _, _ = correlation_pyramid[0].size()\n",
        "  device = correlation_pyramid[0].device\n",
        "\n",
        "  feature_maps = []\n",
        "  # For every level k\n",
        "  for i in range(len(correlation_pyramid)):\n",
        "    # 2 Calculate local grid\n",
        "    corr = correlation_pyramid[i]\n",
        "    corr = corr.reshape(B*W*H, 1, W//2**i, H//2**i) \n",
        "    corr = corr.permute(0, 1, 3, 2) # B*W*Hx1xH/2^ixW/2^i\n",
        "\n",
        "    dx = torch.arange(-radius, radius+1, device=device)\n",
        "    dy = torch.arange(-radius, radius+1, device=device)\n",
        "    delta = torch.stack(torch.meshgrid(dx, dy), axis=-1) # 2x2r+1x2r+1\n",
        "\n",
        "    centroid_lvl = fe.permute(0, 3, 2, 1).reshape(B*W*H, 1, 1, 2) / 2 ** i # B*W*Hx1x1x2\n",
        "    delta_lvl = delta.view(1, 2*radius+1, 2*radius+1, 2) # 1x2r+1x2r+1x2\n",
        "    coords_lvl = centroid_lvl + delta_lvl # B*W*Hx2r+1x2r+1x2\n",
        "\n",
        "    h_corr, w_corr = corr.size()[-2:] #H//2^i , W//2^i\n",
        "    xgrid, ygrid = coords_lvl.split([1, 1], dim=-1) # B*W*Hx2r+1x2r+1x1 , B*W*Hx2r+1x2r+1x1\n",
        "    xgrid = 2 * xgrid / (w_corr - 1) - 1 # B*W*Hx2r+1x2r+1x1\n",
        "    ygrid = 2 * ygrid / (h_corr - 1) - 1 # B*W*Hx2r+1x2r+1x1\n",
        "\n",
        "    grid = torch.cat([xgrid, ygrid], dim=-1) # B*W*Hx2r+1x2r+1x2\n",
        "\n",
        "    # 3 Bilinear interpolation\n",
        "    bilinear_sample = F.grid_sample(corr, grid, align_corners=True)\n",
        "    bilinear_sample = bilinear_sample.view(B, H, W, -1) # BxHxWx(2r+1)**2\n",
        "\n",
        "    feature_maps.append(bilinear_sample)\n",
        "\n",
        "  return torch.cat(feature_maps, dim=-1).permute(0, 3, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyGyPokzMeou"
      },
      "source": [
        "# Iterative Updates\n",
        "* Initialization (flow field = 0)\n",
        "* Inputs (correlation features using previous sections)\n",
        "* Update (GRU cell)\n",
        "* Flow Prediction (delta flow)\n",
        "* Upsampling (final high-res flow field)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOhefX_UqcTm"
      },
      "outputs": [],
      "source": [
        "class UpdateBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_dim=96, radius=4):\n",
        "    super().__init__()\n",
        "    # self.conv1_flow = nn.Conv2d(324, 256, 1, padding=1)\n",
        "    self.conv1_flow = nn.Conv2d(2, 64, 7, padding=3)\n",
        "    self.conv2_flow = nn.Conv2d(64, 32, 3, padding=1)\n",
        "\n",
        "    # input dim is 4*(2r+1)^2\n",
        "    self.conv_corr = nn.Conv2d(4*(2*radius+1)**2, 96, 1, padding=0)\n",
        "    # print(\"update input dimension: \", 4*(2*radius+1)**2)\n",
        "    self.conv_final = nn.Conv2d(128, 80, 3, padding=1)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, flow, corr):\n",
        "    # print(\"[UB]: flow\", flow.size(), \" corr: \", corr.size())\n",
        "    f1 = self.conv1_flow(flow)\n",
        "    r1 = self.relu(f1)\n",
        "    f2 = self.conv2_flow(r1)\n",
        "    flow_output = self.relu(f2)\n",
        "    # print(\"UB: flow_output\", flow_output.size())\n",
        "\n",
        "    corr_output = self.relu(self.conv_corr(corr))\n",
        "    # print(\"UB: corr_output\", corr_output.size())\n",
        "    correlation_flow = torch.cat([flow_output, corr_output], dim=1)\n",
        "\n",
        "    return self.conv_final(correlation_flow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIcJD7pvvtcY"
      },
      "outputs": [],
      "source": [
        "class DeltaFBlock(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(96, 128, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(128, 2, 3, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "  def forward(self, h):\n",
        "    return self.conv2(self.relu(self.conv1(h)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W67ztRqkUUD-"
      },
      "outputs": [],
      "source": [
        "#A custom GRU cell using PyTorch\n",
        "#fully connected layers replaced with following\n",
        "#z_t = σ(Conv3x3([h_(t−1), x_t], W_z))\n",
        "#r_t = σ(Conv3x3([h_(t−1), x_t], W_r))\n",
        "#h˜_t = tanh(Conv3x3([r_t (dot) h_(t−1), x_t], W_h))\n",
        "#h_t = (1 − z_t) (dot) h_(t−1) + z_t (dot) h˜_t\n",
        "class GRUCell(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.conv_z = torch.nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv_r = torch.nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        self.conv_h = torch.nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, padding=1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "        self.tanh = torch.nn.Tanh()\n",
        "\n",
        "        self.mask = nn.Sequential(\n",
        "          nn.Conv2d(96, 4*hidden_size, 3, padding=1),\n",
        "          torch.nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(4*hidden_size, 8*8*9, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # print(\"[GRU] x: \", x.size(), \" h: \", h.size())\n",
        "        z = self.sigmoid(self.conv_z(torch.cat((h, x), dim=1)))\n",
        "        # print(\"[GRU] z: \", z.size())\n",
        "        r = self.sigmoid(self.conv_r(torch.cat((h, x), dim=1)))\n",
        "        # print(\"[GRU] r: \", r.size())\n",
        "        h_hat = self.tanh(self.conv_h(torch.cat((r * h, x), dim=1)))\n",
        "        # print(\"[GRU] h_hat: \", h_hat.size())\n",
        "        h = (1 - z) * h + z * h_hat\n",
        "        # print(\"[GRU] h: \", h.size())\n",
        "        return h, self.mask(h)\n",
        "    def init_hidden(self, batch_size, height, width, device):\n",
        "        return torch.zeros(batch_size, self.hidden_size, height, width, device=device)\n",
        "    '''\n",
        "    #Normally shouldnt be necessary as torch does backpropagation automatically\n",
        "    def backward(self, x, h):\n",
        "        z = self.sigmoid(self.conv_z(torch.cat((h, x), 1)))\n",
        "        r = self.sigmoid(self.conv_r(torch.cat((h, x), 1)))\n",
        "        h_hat = self.tanh(self.conv_h(torch.cat((r * h, x), 1)))\n",
        "        h = (1 - z) * h + z * h_hat\n",
        "        return h\n",
        "    '''\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFBa-kOPT4CU"
      },
      "outputs": [],
      "source": [
        "class RAFT(nn.Module):\n",
        "  def __init__(self, hidden_dim=96, context_dim=64):\n",
        "    super(RAFT, self).__init__()\n",
        "    self.featureEncoder = Encoder()\n",
        "    self.contextEncoder = Encoder(num_output_channels=160)\n",
        "\n",
        "    self.h_dim = hidden_dim\n",
        "    self.c_dim = context_dim\n",
        "    self.gru = GRUCell(146, self.h_dim)\n",
        "\n",
        "    self.update_block = UpdateBlock()\n",
        "    self.delta_fblock = DeltaFBlock()\n",
        "\n",
        "  def upsample_flow(self, mask, flow):\n",
        "    # print(x.shape)\n",
        "    B, D, H, W = flow.shape\n",
        "    # print(mask.shape)\n",
        "    mask = mask.view(B, 1, 9, 8, 8, H, W)\n",
        "    mask = torch.softmax(mask, dim=2)\n",
        "\n",
        "    flow = F.unfold(8*flow, 3, padding=1)\n",
        "    y = flow.view(B, 2, 9, 1, 1, H, W)\n",
        "    # SANDER: same upsampling mode here? I think it has the same issue! (FIXED)\n",
        "    y = torch.sum(mask * y, dim=2)\n",
        "    y = y.permute(0, 1, 4, 2, 5, 3)\n",
        "\n",
        "    return y.reshape(B, 2, 8*H, 8*W)\n",
        "\n",
        "  def forward(self, img1, img2):\n",
        "    iter=12\n",
        "    #print(f\"img1 size = {img1.size()}\")\n",
        "    B, D, H, W = img1.shape\n",
        "        \n",
        "    context_features = self.contextEncoder(img1)\n",
        "    gru_hidden, context = torch.split(context_features, [self.h_dim, self.c_dim], dim=1)\n",
        "    # print(\"hidden: \", self.hidden.size())\n",
        "    # print(\"context: \", self.context.size())\n",
        "\n",
        "    # print(f\"context size = {context.size()}\")\n",
        "    features1 = self.featureEncoder(img1)\n",
        "    # print(f\"features1 size = {features1.size()}\")\n",
        "    features2 = self.featureEncoder(img2)\n",
        "\n",
        "    corr = correlationVolume(features1, features2)\n",
        "    # print(f\"corr size = {corr.size()}\")\n",
        "\n",
        "    pyramids = constructPyramid(corr)\n",
        "    # pyramids = [torch.ones([B, W//8, W//8, W//8, W//8]), torch.ones([B, W//8, W//8, 16, 16]), torch.ones([B, W//8, W//8, 8, 8]), torch.ones([B, W//8, W//8, 4, 4])]\n",
        "    # print(f\"pyramids size = {pyramids[0].size()} {pyramids[1].size()} {pyramids[2].size()} {pyramids[3].size()}\")\n",
        "\n",
        "\n",
        "    flow = torch.zeros(B, 2, H//8, W//8, device=device)\n",
        "    # SANDER: paper predicts initial GRU state from the context encoder (FIXED)\n",
        "    #gru_hidden = self.gru.init_hidden(B, H//8, W//8, device=device)\n",
        "\n",
        "    upscaled_flow_estimates =[]\n",
        "    # Loop over iteration\n",
        "    for i in range(iter):\n",
        "        # Fix feature map\n",
        "        corr = createFeatureMap(pyramids, flow)\n",
        "\n",
        "        # print(\"corr(feature map): \", corr.size());\n",
        "\n",
        "        corr_features = self.update_block(flow, corr)\n",
        "\n",
        "        # print(\"corr_features: \", corr_features.size())\n",
        "        # print(\"context: \", context.size())\n",
        "        # print(\"flow: \", self.flow.size())\n",
        "\n",
        "        cat = torch.cat([corr_features, context, flow], dim=1)\n",
        "\n",
        "        # print(\"cat: \", cat.size())\n",
        "        # print(\"gru_hidden: \", self.gru_hidden.size())\n",
        "\n",
        "        gru_output, mask = self.gru(cat, gru_hidden)\n",
        "\n",
        "        # print(\"gru_output: \", gru_output.size())\n",
        "\n",
        "        delta_output = self.delta_fblock(gru_output)\n",
        "\n",
        "        # print(\"delta_output: \", delta_output.size())\n",
        "\n",
        "        flow = delta_output + flow\n",
        "        upsampled_flow = self.upsample_flow(mask, flow)\n",
        "\n",
        "        upscaled_flow_estimates.append(upsampled_flow)\n",
        "        # upscaled_flow_estimates.append(torch.ones([B, 2, width*8, height*8]))\n",
        "    \n",
        "    return upscaled_flow_estimates\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "ri6_t6b4fLWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "U-UwMD7qrnZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(flow_preds, flow):\n",
        "  gamma = 0.8\n",
        "  total_loss = 0\n",
        "  for i in range(len(flow_preds)):\n",
        "      weight = gamma**(len(flow_preds)-i-1) # SANDER: careful; fails for iters != 12 (FIXED)\n",
        "      iter_loss = (flow_preds[i] - flow).abs()\n",
        "      total_loss += weight*iter_loss.mean()\n",
        "  \n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "boMEO1iC-M99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yN5WefPgKjBP"
      },
      "outputs": [],
      "source": [
        "def train_model(raft):\n",
        "  # train_features, train_labels = next(iter(train_loader))\n",
        "\n",
        "  # train_features = torch.ones(5, 3, 256, 256)\n",
        "\n",
        "  # torch.autograd.set_detect_anomaly(True) # SANDER: careful; a lot slower, make sure to remove for training runs (FIXED)\n",
        "\n",
        "  #print(train_features.shape)\n",
        "\n",
        "  \n",
        "  raft.to(device)\n",
        "  raft.train()\n",
        "\n",
        "  # loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "  scaler = GradScaler()\n",
        "  optimizer= torch.optim.Adam(raft.parameters(), lr=0.0001)\n",
        "\n",
        "  total_epoch = 2\n",
        "  batch_print = 20\n",
        "  for epoch in range(total_epoch):\n",
        "    epoch_start = time.time()\n",
        "\n",
        "    batch_no = 1\n",
        "    total_batch = len(train_loader)\n",
        "    # raft(train_features, train_features)\n",
        "    for image_batch in train_loader:\n",
        "        if batch_no % batch_print == 1:\n",
        "            print('Batch No:', batch_no, 'out of', total_batch)\n",
        "            batch_start = time.time()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        image_batch1, image_batch2, flow = [x.to(device) for x in image_batch]\n",
        "        \n",
        "        B, D, H, W = image_batch1.shape\n",
        "            \n",
        "        # Move tensor to the proper device\n",
        "        # SANDER: using bilinear interpolation on flows usually isn't the best idea; pad the images instead to ensure (size % 8 == 0) (FIXED)\n",
        "\n",
        "        height = int((H+63) // 64)\n",
        "        width = int((W+63) // 64)\n",
        "        \n",
        "        width_padding = (width*64 - W) // 2\n",
        "        height_padding = (height*64 - H) // 2\n",
        "\n",
        "        # SANDER: pad, don't resize :) (FIXED)\n",
        "        image_batch1 = F.pad(image_batch1, pad=(width_padding, width_padding, height_padding, height_padding))\n",
        "        image_batch2 = F.pad(image_batch2, pad=(width_padding, width_padding, height_padding, height_padding))\n",
        "        flow = F.pad(flow, pad=(width_padding, width_padding, height_padding, height_padding))\n",
        "\n",
        "        # Forward pass through network\n",
        "        upscaled_flow_preds = raft(image_batch1, image_batch2)\n",
        "        #upscaled_flow_preds = upscaled_flow_preds.detach()\n",
        "        # Evaluate loss\n",
        "        loss = get_loss(upscaled_flow_preds, flow)\n",
        "        #loss = F.mse_loss(upscaled_flow_preds, flow, reduction=\"none\")\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(raft.parameters(), 1.0)\n",
        "        # Backward pass\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        #optimizer.step()\n",
        "        if batch_no % batch_print == 0:\n",
        "            batch_end = time.time()\n",
        "            print('Batch mean time elapsed:', round((batch_end-batch_start)/batch_print, 2), 's loss:', loss)\n",
        "        batch_no+=1\n",
        "    epoch_end = time.time()\n",
        "    print('Epoch time elapsed:', round(epoch_end-epoch_start, 2),'s with Average batch time:', round((epoch_end-epoch_start)/total_batch, 2))\n",
        "    torch.save(raft, './models/epoch_'+str(epoch+1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raft = RAFT()\n",
        "train_model(raft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkwRFzPXW0DJ",
        "outputId": "714fba0a-a22d-4d36-e157-38f3f3f20560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch No: 1 out of 373\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "Batch mean time elapsed: 63.4 s loss: tensor(33.9471, grad_fn=<AddBackward0>)\n",
            "Batch No: 21 out of 373\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n",
            "img1 size = torch.Size([2, 3, 448, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raft_early = torch.load('./models/epoch_1')"
      ],
      "metadata": {
        "id": "6oDv2vCrGEix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shows sample image flow and calculated image flow for one sample\n",
        "raft.eval()\n",
        "for sample_batch in train_loader:\n",
        "    image_batch1, image_batch2, flow = [x.to(device) for x in sample_batch]\n",
        "    # Move tensor to the proper device\n",
        "    # SANDER: same as before\n",
        "    flow = transforms.functional.resize(flow, size=(int(image_batch1.size()[2] // 8)*8, int(image_batch1.size()[3] // 8)*8), antialias=False)\n",
        "    # Forward pass through network\n",
        "    upscaled_flow_preds = raft(image_batch1, image_batch2)\n",
        "    upscaled_flow_preds_early = raft_early(image_batch1, image_batch2)\n",
        "    _, H, W = flow[0].size()\n",
        "    buffer = torch.zeros(H, W, 1)\n",
        "\n",
        "    # visualize inputs outputs\n",
        "    plt.figure()\n",
        "    plt.title('Images')\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_batch1[0].cpu().permute(1,2,0))\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(image_batch2[0].cpu().permute(1,2,0))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('Flows')\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(torch.cat((buffer,upscaled_flow_preds_early[0][-1].cpu().permute(1,2,0)), dim=-1).detach().numpy())\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(torch.cat((buffer,upscaled_flow_preds[0][-1].cpu().permute(1,2,0)), dim=-1).detach().numpy())\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(torch.cat((buffer,flow[0].cpu().permute(1,2,0)), dim=-1))\n",
        "    break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VJ1ke97jy_Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del raft\n",
        "raft = None\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "aVpaoynTi1_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L  "
      ],
      "metadata": {
        "id": "LP6WkA7akfkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Xu7Jp_t98QWg"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}